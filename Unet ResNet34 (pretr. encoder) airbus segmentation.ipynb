{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Input paths"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\nPRETRAINED_ENCODER_PATH = '../input/airbus-ship-detection-resnet34-256px'\nPRETRAINED_ENCODER_NAME = 'ResNet34_ship_detection_256_4.h5'\n\n# PRETRAINED_SEG_MODEL_PATH = '../input/airbus-seg-model-256-sr-1/seg_model_256_ships_ratio_1.hdf5'\nPRETRAINED_SEG_MODEL_PATH = '../input/airbus-seg-model-256-sr-0741/seg_model_768_ships_ratio_0741_5hr.hdf5'\n\nSHIP_DIR = '../input/airbus-ship-detection'\nTRAIN_IMAGE_DIR = os.path.join(SHIP_DIR, 'train_v2')\nTEST_IMAGE_DIR = os.path.join(SHIP_DIR, 'test_v2')\n\nMASKS_DF = 'train_ship_segmentations_v2.csv'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"masks = pd.read_csv(os.path.join(SHIP_DIR, MASKS_DF))\nprint(masks.shape[0], 'masks found')\nprint(masks['ImageId'].value_counts().shape[0])\nmasks.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6cd9d5ad61ffe3b8858769f20a5f9493f024a56"},"cell_type":"markdown","source":"## Model Parameters\nWe might want to adjust these later (or do some hyperparameter optimizations)"},{"metadata":{"trusted":true,"_uuid":"301a5d939c566d1487a049bb2554d09b592b18b1"},"cell_type":"code","source":"BATCH_SIZE = 4\nEDGE_CROP = 16\nNB_EPOCHS = 5\nGAUSSIAN_NOISE = 0.1\nUPSAMPLE_MODE = 'SIMPLE'\n# number of validation images to use\nVALID_IMG_COUNT = 400\n# maximum number of steps_per_epoch in training\nMAX_TRAIN_STEPS = 200\nAUGMENT_BRIGHTNESS = False\n\nIMG_SIZE = 768","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c826bd059a97f34ef3549a393d140dd1451d657"},"cell_type":"code","source":"!ls ../input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import imgaug\n# imgaug.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --upgrade imgaug","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# from skimage.io import imread\nfrom cv2 import imread \nimport matplotlib.pyplot as plt\nfrom skimage.segmentation import mark_boundaries\nimport time\n\nfrom keras.utils import Sequence\nfrom sklearn.metrics import fbeta_score\nfrom imgaug import augmenters as iaa\n\nimport cv2\n\nfrom skimage.util import montage\n# from skimage.util.montage2d import montage2d as montage was this for older skimage\n\nmontage_rgb = lambda x: np.stack([montage(x[:, :, :, i]) for i in range(x.shape[3])], -1)\n\nimport gc; gc.enable() # memory is tight\n\nfrom skimage.morphology import label\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef multi_rle_encode(img):\n    labels = label(img[:, :, 0])\n    return [rle_encode(labels==k) for k in np.unique(labels[labels>0])]\n\n# ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\ndef rle_encode(img):\n    '''\n    Returns run length as string formated\n    '''\n    pixels = img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\ndef rle_decode(mask_rle, shape=(768, 768)):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape).T  # Needed to align to RLE direction\n\ndef masks_as_image(in_mask_list):\n    # Take the individual ship masks and create a single mask array for all ships\n    all_masks = np.zeros((768, 768), dtype = np.int16)\n    #if isinstance(in_mask_list, list):\n    for mask in in_mask_list:\n        if isinstance(mask, str):\n            all_masks += rle_decode(mask)\n    return np.expand_dims(all_masks, -1)\n\ndef masks_as_image_to_dict(row):\n    in_mask_list = [row['EncodedPixels']]\n    image_name = row['ImageId']\n    # Take the individual ship masks and create a single mask array for all ships\n    all_masks = np.zeros((768, 768), dtype = np.int16)\n    #if isinstance(in_mask_list, list):\n    for mask in in_mask_list:\n        if isinstance(mask, str):\n            all_masks += rle_decode(mask)\n            \n    mask = np.expand_dims(all_masks, -1)\n    try:\n        dict_masks[image_name].append(mask)\n    except:\n        dict_masks[image_name] = [mask]\n#     return np.expand_dims(all_masks, -1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40cb72e241c0c3d8bc245b4e3c663b4a835b0011"},"cell_type":"markdown","source":"# Split into training and validation groups\nWe stratify by the number of boats appearing so we have nice balances in each set"},{"metadata":{"trusted":true,"_uuid":"c4f008bf6898518fd371de013418f936edaa09f8"},"cell_type":"code","source":"masks['ships'] = masks['EncodedPixels'].map(lambda c_row: 1 if isinstance(c_row, str) else 0)\nunique_img_ids = masks.groupby('ImageId').agg({'ships': 'sum'}).reset_index()\nunique_img_ids['has_ship'] = unique_img_ids['ships'].map(lambda x: 1.0 if x>0 else 0.0)\nunique_img_ids['has_ship_vec'] = unique_img_ids['has_ship'].map(lambda x: [x])\n# some files are too small/corrupt\nunique_img_ids['file_size_kb'] = unique_img_ids['ImageId'].map(lambda c_img_id: \n                                                               os.stat(os.path.join(TRAIN_IMAGE_DIR, \n                                                                                    c_img_id)).st_size/1024)\nunique_img_ids = unique_img_ids[unique_img_ids['file_size_kb']>50] # keep only 50kb files\nunique_img_ids['file_size_kb'].hist()\nmasks.drop(['ships'], axis=1, inplace=True)\nunique_img_ids.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"871720221ac25f7f9408bfe01aeb4ccb95edbd1f"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_ids, valid_ids = train_test_split(unique_img_ids, \n                 test_size = 0.05, \n                 stratify = unique_img_ids['ships'])\ntrain_df = pd.merge(masks, train_ids)\nvalid_df = pd.merge(masks, valid_ids)\nprint(train_df.shape[0], 'training masks')\nprint(valid_df.shape[0], 'validation masks')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Balance whole train_df dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# seg_df_copy = masks.copy()    \nseg_df = train_df\n\n\nships_ratio_all = 0.6  ## WRONG coef  (0.5 min -> more like non_ship/ship ratio)  0.65=0.76369    ////// HERE LABELS 0.6=0.84685!!0.6=0.741 ship/no_ship images\nship_images_cnt = seg_df[seg_df['has_ship'] == 1].shape[0]\nx_coef = seg_df.shape[0]/ship_images_cnt - (1/ships_ratio_all) # just for formula\n\nexclude_non_ship_images_cnt = seg_df.shape[0]-int(x_coef*ship_images_cnt)\nlist_ids_exclude = seg_df[seg_df['has_ship']!= 1].sample(exclude_non_ship_images_cnt)['ImageId']\nseg_df = seg_df[~seg_df['ImageId'].isin(list_ids_exclude)]\nprint(\"ship labels (few for multiple ships per image):\", seg_df[seg_df['has_ship'] == 1]['ImageId'].nunique())\nprint(\"non ship labels:\", seg_df[seg_df['has_ship'] != 1].shape[0])\nprint(\"Proportion has_ship=1:\", seg_df[seg_df['has_ship'] == 1]['ImageId'].nunique()/seg_df['ImageId'].nunique())\ntrain_df = seg_df\ntrain_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # train_df = train_df.sample(40*1000)\n# train_df = train_df[:20000]\n# print(\"ship labels (few for multiple ships per image):\", train_df[train_df['has_ship'] == 1].shape[0])\n# print(\"non ship labels:\", train_df[train_df['has_ship'] != 1].shape[0])\n# train_df[train_df['has_ship'] == 1]['ImageId'].nunique()/train_df['ImageId'].nunique()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0cf0bb261eda957cb0a12a330260e1390c57c8c9"},"cell_type":"code","source":"# train_df['grouped_ship_count'] = train_df['ships'].map(lambda x: (x+1)//2).clip(0, 7)\n# def sample_ships(in_df, base_rep_val=1500):\n#     if in_df['ships'].values[0]==0:\n#         return in_df.sample(base_rep_val//3) # even more strongly undersample no ships\n#     else:\n#         return in_df.sample(base_rep_val, replace=(in_df.shape[0]<base_rep_val))\n    \n# balanced_train_df = train_df.groupby('grouped_ship_count').apply(sample_ships)\n# balanced_train_df['ships'].hist(bins=np.arange(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1282446dc1a67c4c50ad220703a18973135aac4"},"cell_type":"code","source":"# balanced_train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import keras\n\n\n\nclass DataGenerator(Sequence):\n    'Generates data for Keras'\n    def __init__(self, in_df, is_validation=False, ships_ratio=None, batch_size=32, crop_size=256, shuffle=True):\n        'Initialization'\n        self.in_df = in_df\n        self.is_validation = is_validation\n        self.ships_ratio = ships_ratio \n        self.batch_size = batch_size\n        self.crop_size = crop_size\n        self.shuffle = shuffle\n        \n        if ships_ratio == 1:\n            #only has_ship images\n            self.in_df = in_df[in_df['has_ship']==1.0]\n        if ships_ratio is None:\n            #then assign default ship/no_ship distribution\n            self.ships_ratio = in_df[in_df['has_ship']==1.0]['ImageId'].nunique()/in_df['ImageId'].nunique()\n        self.list_IDs = self.in_df['ImageId'].unique()\n\n        print(\"is_validation: {}, ships_ratio:{}\".format(is_validation, round(self.ships_ratio, 3)))\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) / self.batch_size))\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n            \n    def augmentor(self, X, y):\n        \"\"\"\n        dg_args = dict(featurewise_center = False, \n                  samplewise_center = False,\n                  rotation_range = 15, ---------\n                  width_shift_range = 0.1, \n                  height_shift_range = 0.1, \n                  shear_range = 0.01,\n                  zoom_range = [0.9, 1.25],  \n                  horizontal_flip = True, \n                  vertical_flip = True,\n                  fill_mode = 'reflect',\n                   data_format = 'channels_last')\n        \"\"\"      \n        geometric_augs = [\n            iaa.Fliplr(0.5), # horizontally flip 50% of all images\n            iaa.Flipud(0.3), # vertically flip 30% of all images\n            iaa.Affine(rotate=(-15,15)) # random rotate on +-15 degree\n        ]\n        visual_augs = [\n            iaa.Add((-5, 5), per_channel=0.5), # change brightness of images\n            iaa.AdditiveGaussianNoise(scale=0.02*255),\n            iaa.GaussianBlur(sigma=[0, 0, 0, 0.01, 0.05, 0.2, 0.5, 0.8, 1, 1.5])\n        ]\n        seq_aug = iaa.Sequential(geometric_augs+visual_augs)\n        X, y = seq_aug(images=X, segmentation_maps=y)\n        return X, y\n            \n    def _get_smart_cropped_image_mask(self, c_img, c_mask, c_masks):\n        \"\"\"\n        Function takes:\n        @c_img: input fullsize rgb image (3 dims)\n        @c_mask: input fullsize mask (1 dim)\n        @c_masks: dataframe with ships' masks form this image\n        (takes 3-4ms for image with ships) an\n        d ~0.1ms for empty image\n        \"\"\"\n        crop_margin = int(self.crop_size/2)\n#         try:\n        if c_masks['has_ship'].values[0]==1:\n            #pick random ship from image\n            c_mask_ship = masks_as_image(np.array([np.random.choice(c_masks['EncodedPixels'].values)]))\n            mask_ship_bw = c_mask_ship[:,:,0]\n            #find indexes where mask is 1\n            arr_indexes = np.where(mask_ship_bw == 1)\n            center_x = int((arr_indexes[1].min()+arr_indexes[1].max())/2)\n            center_y = int((arr_indexes[0].min()+arr_indexes[0].max())/2)\n        else:\n            center_x = np.random.uniform(0, IMG_SIZE)\n            center_y = np.random.uniform(0, IMG_SIZE)\n        # we got ~ center of picked ship\n\n        #make ship position uniformly distributed in image crop, not only at the center\n        #minimum half of ship is present in image\n        center_x = int(center_x+(crop_margin*np.random.uniform(low=-0.5, high=0.5)))\n        center_y = int(center_y+(crop_margin*np.random.uniform(low=-0.5, high=0.5)))\n\n        #get center of crop understanding border of the image\n        center_crop_x = center_x\n        center_crop_y = center_y\n        if center_x-crop_margin<0:\n            center_crop_x = crop_margin\n        if center_x+crop_margin>IMG_SIZE:\n            center_crop_x = IMG_SIZE-crop_margin\n        if center_y-crop_margin<0:\n            center_crop_y = crop_margin\n        if center_y+crop_margin>IMG_SIZE:\n            center_crop_y = IMG_SIZE-crop_margin\n\n        #final crope coordinates\n        x1 = center_crop_x-crop_margin\n        x2 = center_crop_x+crop_margin\n        y1 = center_crop_y-crop_margin\n        y2 = center_crop_y+crop_margin\n\n        c_img  =  c_img[y1:y2,x1:x2,:]\n        c_mask = c_mask[y1:y2,x1:x2,:]\n        return c_img, c_mask\n\n\n    def __data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.empty((self.batch_size, self.crop_size, self.crop_size, 3), dtype='float32')\n        y = np.empty((self.batch_size, self.crop_size, self.crop_size, 1), dtype='uint8')\n        \n        # Generate data\n        for i, c_img_id in enumerate(list_IDs_temp):\n            c_masks = self.in_df[self.in_df['ImageId']==c_img_id]\n            \n            rgb_path = os.path.join(TRAIN_IMAGE_DIR, c_img_id)\n            c_img = imread(rgb_path)  # 13ms\n            \n            c_mask = masks_as_image(c_masks['EncodedPixels'].values)\n            \n            #smart image crop\n            if self.crop_size != 768:\n                c_img, c_mask = self._get_smart_cropped_image_mask(c_img, c_mask, c_masks) # 1-4ms\n            \n            # Store sample\n            X[i,] = c_img\n            # Store class\n            y[i] = c_mask\n#             print(\"time for item: ms\", 1000*(time.time()-time_start_item))  # 35ms\n\n        return X, y\n    \n    def __getitem__(self, index):\n        time_start_batch = time.time()\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n\n        # Generate data\n        X, y = self.__data_generation(list_IDs_temp)  # ~1.5s\n\n        # preprocess and augment data\n#         time_start_aug = time.time()\n        if not self.is_validation:\n            X, y = self.augmentor(X, y)  # 0.001 ms\n#         print(\"\\n\\ntime for aug: ms\", 1000*(time.time()-time_start_aug))\n\n#         print(\"\\n\\n\\ntime for batch: ms\", 1000*(time.time()-time_start_batch))\n        return X/255.0, y\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training_generator = DataGenerator(train_df, is_validation=False, ships_ratio=1, batch_size=32, crop_size=256, shuffle=True)\n# X, y = next(iter(training_generator))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from keras.preprocessing.image import ImageDataGenerator\n\n# dg_args = dict(featurewise_center = False, \n#                   samplewise_center = False,\n#                   rotation_range = 15, \n#                   width_shift_range = 0.1, \n#                   height_shift_range = 0.1, \n#                   shear_range = 0.01,\n#                   zoom_range = [0.9, 1.25],  \n#                   horizontal_flip = True, \n#                   vertical_flip = True,\n#                   fill_mode = 'reflect',\n#                    data_format = 'channels_last')\n# # brightness can be problematic since it seems to change the labels differently from the images \n# if AUGMENT_BRIGHTNESS:\n#     dg_args[' brightness_range'] = [0.5, 1.5]\n# image_gen = ImageDataGenerator(**dg_args)\n\n# if AUGMENT_BRIGHTNESS:\n#     dg_args.pop('brightness_range')\n# label_gen = ImageDataGenerator(**dg_args)\n\n# def create_aug_generator(in_gen, seed = 42):\n#     \"\"\"\n#     Wrapper around previous generator that streams cropped batched images\n#     and make data augmentation on images\n#     \"\"\"\n#     np.random.seed(seed if seed is not None else np.random.choice(range(9999)))\n#     for in_x, in_y in in_gen:\n#         seed = np.random.choice(range(9999))\n#         # keep the seeds syncronized otherwise the augmentation to the images is different from the masks\n#         g_x = image_gen.flow(255*in_x, \n#                              batch_size = in_x.shape[0], \n#                              seed = seed, \n#                              shuffle=True)\n#         g_y = label_gen.flow(in_y, \n#                              batch_size = in_x.shape[0], \n#                              seed = seed, \n#                              shuffle=True)\n\n#         yield next(g_x)/255.0, next(g_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def change_model_input_shape(model, loss, lr, new_input_shape=(None, 768, 768, 3)):\n    # replace input shape of first layer\n    model._layers[0].batch_input_shape = new_input_shape\n\n    # rebuild model architecture by exporting and importing via json\n    new_model = keras.models.model_from_json(model.to_json())\n\n    # copy weights from old model to new one\n    for layer in new_model.layers:\n        try:\n            layer.set_weights(model.get_layer(name=layer.name).get_weights())\n        except:\n            print(\"Could not transfer weights for layer {}\".format(layer.name))\n    \n    # train\n    del model\n#     adam = Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999, amsgrad=False)\n    optimizer = Adam(lr, decay=1e-6)\n#     optimizer = SGD(learning_rate=lr)\n    new_model.compile(optimizer=optimizer, loss=loss, metrics=[dice_coef, 'binary_accuracy', true_positive_rate])\n    return new_model\n    \ndef change_input_shape(model, loss, lr=0.0001, new_input_size=256, batch_size=64, ships_ratio=None):\n    print(\"Changing model's input shape to:\", new_input_size)\n    model = change_model_input_shape(model, loss, lr=lr, new_input_shape=(None, new_input_size, new_input_size, 3))\n    print(\"Creating datagenerators...\")\n\n#     train_generator = create_aug_generator(make_image_gen(train_df, batch_size, new_input_size, ships_ratio))\n    train_generator = DataGenerator(train_df, is_validation=False, ships_ratio=ships_ratio, batch_size=batch_size, crop_size=new_input_size, shuffle=True)\n    validation_generator = DataGenerator(valid_df, is_validation=True, ships_ratio=ships_ratio, batch_size=batch_size, crop_size=new_input_size)\n#     validation_generator = next(make_image_gen(valid_df, valid_df['ImageId'].nunique(), crop_size=new_input_size, ships_ratio=None))\n\n    \n    return model, train_generator, validation_generator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33300c4f03b6600da7b418f775d11d7ebf76a35a"},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba08494eb9736ec3556b7c879143cdcdea89febf"},"cell_type":"markdown","source":"# Build ResNet34+ Unet Model\nHere we use a slight deviation on the U-Net standard"},{"metadata":{"trusted":true,"_uuid":"bc694385625f3a7a49372e211f7298436d027ee4"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport six\n\nfrom random import randint\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\n\nfrom sklearn.model_selection import train_test_split\n\nfrom skimage.transform import resize\n\n\nfrom keras import Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.models import load_model\nfrom keras.optimizers import Adam, SGD\nfrom keras.utils.vis_utils import plot_model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout,BatchNormalization\nfrom keras.layers import Conv2D, Concatenate, MaxPooling2D\nfrom keras.layers import UpSampling2D, Dropout, BatchNormalization\nfrom tqdm import tqdm_notebook\nfrom keras import initializers\nfrom keras import regularizers\nfrom keras import constraints\nfrom keras.utils import conv_utils\nfrom keras.utils.data_utils import get_file\nfrom keras.engine.topology import get_source_inputs\nfrom keras.engine import InputSpec\nfrom keras import backend as K\nfrom keras.regularizers import l2\n\nfrom keras.engine.topology import Input\nfrom keras.engine.training import Model\nfrom keras.layers.convolutional import Conv2D, UpSampling2D, Conv2DTranspose\nfrom keras.layers.core import Activation, SpatialDropout2D\nfrom keras.layers.merge import concatenate,add\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.pooling import MaxPooling2D","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e54a6b4396f146c1663f3ecf4196e6d6e4a0a2a3"},"cell_type":"markdown","source":"## Create Upsample layer\n"},{"metadata":{"trusted":true,"_uuid":"ed9721b45b3ea28752cb0c685ceef80451dec4c0"},"cell_type":"code","source":"from keras.layers import Conv2DTranspose\nfrom keras.layers import UpSampling2D\nfrom keras.layers import Conv2D\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Activation\nfrom keras.layers import Concatenate\n\ndef handle_block_names_decode(stage):\n    conv_name = 'decoder_stage{}_conv'.format(stage)\n    bn_name = 'decoder_stage{}_bn'.format(stage)\n    relu_name = 'decoder_stage{}_relu'.format(stage)\n    up_name = 'decoder_stage{}_upsample'.format(stage)\n    return conv_name, bn_name, relu_name, up_name\n\n\ndef Upsample2D_block(filters, stage, kernel_size=(3,3), upsample_rate=(2,2),\n                     batchnorm=False, skip=None):\n\n    def layer(input_tensor):\n\n        conv_name, bn_name, relu_name, up_name = handle_block_names_decode(stage)\n\n        x = UpSampling2D(size=upsample_rate, name=up_name)(input_tensor)\n\n        if skip is not None:\n            x = Concatenate()([x, skip])\n\n        x = Conv2D(filters, kernel_size, padding='same', name=conv_name+'1')(x)\n        if batchnorm:\n            x = BatchNormalization(name=bn_name+'1')(x)\n        x = Activation('relu', name=relu_name+'1')(x)\n\n        x = Conv2D(filters, kernel_size, padding='same', name=conv_name+'2')(x)\n        if batchnorm:\n            x = BatchNormalization(name=bn_name+'2')(x)\n        x = Activation('relu', name=relu_name+'2')(x)\n\n        return x\n    return layer\n\n\ndef Transpose2D_block(filters, stage, kernel_size=(3,3), upsample_rate=(2,2),\n                      transpose_kernel_size=(4,4), batchnorm=False, skip=None):\n\n    def layer(input_tensor):\n\n        conv_name, bn_name, relu_name, up_name = handle_block_names_decode(stage)\n\n        x = Conv2DTranspose(filters, transpose_kernel_size, strides=upsample_rate,\n                            padding='same', name=up_name)(input_tensor)\n        if batchnorm:\n            x = BatchNormalization(name=bn_name+'1')(x)\n        x = Activation('relu', name=relu_name+'1')(x)\n\n        if skip is not None:\n            x = Concatenate()([x, skip])\n\n        x = Conv2D(filters, kernel_size, padding='same', name=conv_name+'2')(x)\n        if batchnorm:\n            x = BatchNormalization(name=bn_name+'2')(x)\n        x = Activation('relu', name=relu_name+'2')(x)\n\n        return x\n    return layer","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a3e6f145db7c5eb92fe521c7e32f5d4fe8aadb9"},"cell_type":"markdown","source":"## Define Unet model (Decoder)"},{"metadata":{"trusted":true,"_uuid":"e22ae327bed4fbb6fcf604b8755f61f2efbf066e"},"cell_type":"code","source":"def build_unet(backbone, classes, last_block_filters, skip_layers,\n               n_upsample_blocks=5, upsample_rates=(2,2,2,2,2),\n               block_type='upsampling', activation='sigmoid',\n               **kwargs):\n\n    backbone_input = backbone.input\n    x = backbone.output\n#     print(\"backbone.output:\", x)\n    if block_type == 'transpose':\n        up_block = Transpose2D_block\n    else:\n        up_block = Upsample2D_block\n\n    # convert layer names to indices\n    skip_layers = ([get_layer_number(backbone, l) if isinstance(l, str) else l\n                    for l in skip_layers])\n    for i in range(n_upsample_blocks):\n\n        # check if there is a skip connection\n        if i < len(skip_layers):\n            print(backbone.layers[skip_layers[i]])\n            print(backbone.layers[skip_layers[i]].output)\n            skip = backbone.layers[skip_layers[i]].output\n        else:\n            skip = None\n\n        up_size = (upsample_rates[i], upsample_rates[i])\n        filters = last_block_filters * 2**(n_upsample_blocks-(i+1))\n\n        x = up_block(filters, i, upsample_rate=up_size, skip=skip, **kwargs)(x)\n\n    print(\"created upsample blocks\")\n    if classes < 2:\n        activation = 'sigmoid'\n\n    x = Conv2D(classes, (3,3), padding='same', name='final_conv')(x)\n    x = Activation(activation, name=activation)(x)\n\n    print(\"creating Model instance\")\n    model = Model(backbone_input, x)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53ffd058a6d40cfd9036146f82467fe3bd0fc2b3"},"cell_type":"markdown","source":"## Built ResNet34 model\n* Reference this [github](https://github.com/qubvel/classification_models/blob/master/classification_models/resnet/)"},{"metadata":{"trusted":true,"_uuid":"5334dd03afa3f361b1c3226916fbe6dbaabe85fb"},"cell_type":"code","source":"from keras.layers import Conv2D\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Activation\nfrom keras.layers import Add\nfrom keras.layers import ZeroPadding2D\n\ndef handle_block_names(stage, block):\n    name_base = 'stage{}_unit{}_'.format(stage + 1, block + 1)\n    conv_name = name_base + 'conv'\n    bn_name = name_base + 'bn'\n    relu_name = name_base + 'relu'\n    sc_name = name_base + 'sc'\n    return conv_name, bn_name, relu_name, sc_name\n\n\ndef basic_identity_block(filters, stage, block):\n\n    def layer(input_tensor):\n        conv_params = get_conv_params()\n        bn_params = get_bn_params()\n        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n\n        x = BatchNormalization(name=bn_name + '1', **bn_params)(input_tensor)\n        x = Activation('relu', name=relu_name + '1')(x)\n        x = ZeroPadding2D(padding=(1, 1))(x)\n        x = Conv2D(filters, (3, 3), name=conv_name + '1', **conv_params)(x)\n\n        x = BatchNormalization(name=bn_name + '2', **bn_params)(x)\n        x = Activation('relu', name=relu_name + '2')(x)\n        x = ZeroPadding2D(padding=(1, 1))(x)\n        x = Conv2D(filters, (3, 3), name=conv_name + '2', **conv_params)(x)\n\n        x = Add()([x, input_tensor])\n        return x\n\n    return layer\n\n\ndef basic_conv_block(filters, stage, block, strides=(2, 2)):\n\n    def layer(input_tensor):\n        conv_params = get_conv_params()\n        bn_params = get_bn_params()\n        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n\n        x = BatchNormalization(name=bn_name + '1', **bn_params)(input_tensor)\n        x = Activation('relu', name=relu_name + '1')(x)\n        shortcut = x\n        x = ZeroPadding2D(padding=(1, 1))(x)\n        x = Conv2D(filters, (3, 3), strides=strides, name=conv_name + '1', **conv_params)(x)\n\n        x = BatchNormalization(name=bn_name + '2', **bn_params)(x)\n        x = Activation('relu', name=relu_name + '2')(x)\n        x = ZeroPadding2D(padding=(1, 1))(x)\n        x = Conv2D(filters, (3, 3), name=conv_name + '2', **conv_params)(x)\n\n        shortcut = Conv2D(filters, (1, 1), name=sc_name, strides=strides, **conv_params)(shortcut)\n        x = Add()([x, shortcut])\n        return x\n\n    return layer\n\n\ndef conv_block(filters, stage, block, strides=(2, 2)):\n    def layer(input_tensor):\n        conv_params = get_conv_params()\n        bn_params = get_bn_params()\n        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n\n        x = BatchNormalization(name=bn_name + '1', **bn_params)(input_tensor)\n        x = Activation('relu', name=relu_name + '1')(x)\n        shortcut = x\n        x = Conv2D(filters, (1, 1), name=conv_name + '1', **conv_params)(x)\n\n        x = BatchNormalization(name=bn_name + '2', **bn_params)(x)\n        x = Activation('relu', name=relu_name + '2')(x)\n        x = ZeroPadding2D(padding=(1, 1))(x)\n        x = Conv2D(filters, (3, 3), strides=strides, name=conv_name + '2', **conv_params)(x)\n\n        x = BatchNormalization(name=bn_name + '3', **bn_params)(x)\n        x = Activation('relu', name=relu_name + '3')(x)\n        x = Conv2D(filters*4, (1, 1), name=conv_name + '3', **conv_params)(x)\n\n        shortcut = Conv2D(filters*4, (1, 1), name=sc_name, strides=strides, **conv_params)(shortcut)\n        x = Add()([x, shortcut])\n        return x\n\n    return layer\n\n\ndef identity_block(filters, stage, block):\n\n    def layer(input_tensor):\n        conv_params = get_conv_params()\n        bn_params = get_bn_params()\n        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n\n        x = BatchNormalization(name=bn_name + '1', **bn_params)(input_tensor)\n        x = Activation('relu', name=relu_name + '1')(x)\n        x = Conv2D(filters, (1, 1), name=conv_name + '1', **conv_params)(x)\n\n        x = BatchNormalization(name=bn_name + '2', **bn_params)(x)\n        x = Activation('relu', name=relu_name + '2')(x)\n        x = ZeroPadding2D(padding=(1, 1))(x)\n        x = Conv2D(filters, (3, 3), name=conv_name + '2', **conv_params)(x)\n\n        x = BatchNormalization(name=bn_name + '3', **bn_params)(x)\n        x = Activation('relu', name=relu_name + '3')(x)\n        x = Conv2D(filters*4, (1, 1), name=conv_name + '3', **conv_params)(x)\n\n        x = Add()([x, input_tensor])\n        return x\n\n    return layer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"619b56e2a6480fcd094c8fa3711e9e7fb2ce1857"},"cell_type":"code","source":"def get_conv_params(**params):\n    default_conv_params = {\n        'kernel_initializer': 'glorot_uniform',\n        'use_bias': False,\n        'padding': 'valid',\n    }\n    default_conv_params.update(params)\n    return default_conv_params\n\n\ndef get_bn_params(**params):\n    default_bn_params = {\n        'axis': 3,\n        'momentum': 0.99,\n        'epsilon': 2e-5,\n        'center': True,\n        'scale': True,\n    }\n    default_bn_params.update(params)\n    return default_bn_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f43c24643a591b5a15280e64a2d343f543eaad0"},"cell_type":"code","source":"import keras.backend as K\nfrom keras.layers import Input\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Activation\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.layers import ZeroPadding2D\nfrom keras.layers import Dense\nfrom keras.models import Model\nfrom keras.engine import get_source_inputs\n\nimport keras\nfrom distutils.version import StrictVersion\n\nif StrictVersion(keras.__version__) < StrictVersion('2.2.0'):\n    from keras.applications.imagenet_utils import _obtain_input_shape\nelse:\n    from keras_applications.imagenet_utils import _obtain_input_shape\n    \ndef build_resnet(\n     repetitions=(2, 2, 2, 2),\n     include_top=True,\n     input_tensor=None,\n     input_shape=None,\n     classes=1000,\n     block_type='usual'):\n\n    # Determine proper input shape\n    input_shape = _obtain_input_shape(input_shape,\n                                      default_size=224,\n                                      min_size=197,\n                                      data_format='channels_last',\n                                      require_flatten=include_top)\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape, name='data')\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor, shape=input_shape)\n        else:\n            img_input = input_tensor\n    \n    # get parameters for model layers\n    no_scale_bn_params = get_bn_params(scale=False)\n    bn_params = get_bn_params()\n    conv_params = get_conv_params()\n    init_filters = 64\n\n    if block_type == 'basic':\n        conv_block = basic_conv_block\n        identity_block = basic_identity_block\n    else:\n        conv_block = usual_conv_block\n        identity_block = usual_identity_block\n    \n    # resnet bottom\n    x = BatchNormalization(name='bn_data', **no_scale_bn_params)(img_input)\n    x = ZeroPadding2D(padding=(3, 3))(x)\n    x = Conv2D(init_filters, (7, 7), strides=(2, 2), name='conv0', **conv_params)(x)\n    x = BatchNormalization(name='bn0', **bn_params)(x)\n    x = Activation('relu', name='relu0')(x)\n    x = ZeroPadding2D(padding=(1, 1))(x)\n    x = MaxPooling2D((3, 3), strides=(2, 2), padding='valid', name='pooling0')(x)\n    \n    # resnet body\n    for stage, rep in enumerate(repetitions):\n        for block in range(rep):\n            \n            filters = init_filters * (2**stage)\n            \n            # first block of first stage without strides because we have maxpooling before\n            if block == 0 and stage == 0:\n                x = conv_block(filters, stage, block, strides=(1, 1))(x)\n                \n            elif block == 0:\n                x = conv_block(filters, stage, block, strides=(2, 2))(x)\n                \n            else:\n                x = identity_block(filters, stage, block)(x)\n                \n    x = BatchNormalization(name='bn1', **bn_params)(x)\n    x = Activation('relu', name='relu1')(x)\n\n    # resnet top\n    if include_top:\n        x = GlobalAveragePooling2D(name='pool1')(x)\n        x = Dense(classes, name='fc1')(x)\n        x = Activation('softmax', name='softmax')(x)\n\n    # Ensure that the model takes into account any potential predecessors of `input_tensor`.\n    if input_tensor is not None:\n        inputs = get_source_inputs(input_tensor)\n    else:\n        inputs = img_input\n        \n    # Create model.\n    model = Model(inputs, x)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a37cb24b97c790f0884d864711409b2e7b0f41f"},"cell_type":"markdown","source":"## Load pretrain Weight by using ImageNet trained\n* Reference this [GITHUB](https://github.com/qubvel/classification_models/blob/9e438e2e133897b115148c737abdda3e1db31787/classification_models/weights.py)"},{"metadata":{"trusted":true,"_uuid":"5c1b6f8ccfe706af1f7b64576910ea97168cc8fd"},"cell_type":"code","source":"def load_model_weights(model):\n    weights_path = os.path.join(PRETRAINED_ENCODER_PATH, PRETRAINED_ENCODER_NAME)\n\n    from keras.models import load_model\n    pretrained_resnet34 = load_model(weights_path)\n    # transfer encoder weights without 3 last layers (dropout, pooling, head:1 sigmoid)\n    for i, layer in enumerate(model.layers[:-3]):\n        try:\n            layer.set_weights(pretrained_resnet34.layers[i].get_weights())\n        except:\n            print(\"Could not transfer weights for layer {}\".format(layer.name))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e3cf49751c8ea27a1cc623263d1781ff261cfb3"},"cell_type":"markdown","source":"\n## Buil Unet model base on ResNet34"},{"metadata":{"trusted":true,"_uuid":"b9f9fe78bb02e8ab116e94aaf540ae4a578b5f8b"},"cell_type":"code","source":"#Freeze Encoder weight if needed\n\ndef freeze_model(model):\n    for layer in model.layers:\n        layer.trainable = False\n    return\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f95ddf698c2afd39efa89a79fe5a436fada456a"},"cell_type":"code","source":"def UResNet34(input_shape=(None, None, 3), classes=1, decoder_filters=16, decoder_block_type='upsampling',\n                       encoder_weights=None, input_tensor=None, activation='sigmoid', **kwargs):\n\n    backbone = build_resnet(input_tensor=None,\n                         input_shape=input_shape,\n                         repetitions=(3, 4, 6, 3),\n                         classes=classes,\n                         include_top=False,\n                         block_type='basic')\n    backbone.name = 'resnet34'\n    \n    if encoder_weights == True:\n        load_model_weights(backbone)\n        print(\"Loaded weight for the encoder!\")\n    \n    skip_connections = list([129, 74, 37, 5]) # for resnet 34\n    model = build_unet(backbone, classes, decoder_filters,\n                       skip_connections, block_type=decoder_block_type,\n                       activation=activation, **kwargs)\n    model.name = 'u-resnet34'\n    \n#     print(\"Freezing backbone model's layers\")\n#     freeze_model(backbone)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4710a5f4ebbb37d979acb0e9ddb18a0284d8e1e7"},"cell_type":"markdown","source":"## Define Loss function"},{"metadata":{"trusted":true,"_uuid":"4591a1e8577fb7a95a0c771d0ead24380dbd423a"},"cell_type":"code","source":"from keras.losses import binary_crossentropy\nfrom keras import backend as K\n\n\"\"\"\nHere is a dice loss for keras which is smoothed to approximate a linear (L1) loss.\nIt ranges from 1 to 0 (no error), and returns results similar to binary crossentropy\n\"\"\"\ndef dice_coef(y_true, y_pred, smooth=1):\n    \"\"\"\n    Dice = (2*|X & Y|)/ (|X|+ |Y|)\n         =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))\n    ref: https://arxiv.org/pdf/1606.04797v1.pdf\n    \"\"\"\n    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n    return (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)\ndef dice_coef_loss(y_true, y_pred):\n    return 1-dice_coef(y_true, y_pred)\n\ndef dice_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred = K.cast(y_pred, 'float32')\n    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n    intersection = y_true_f * y_pred_f\n    score = 2. * K.sum(intersection) / (K.sum(y_true_f) + K.sum(y_pred_f))\n    return score\n\ndef dice_loss(y_true, y_pred):\n    return dice_coef_loss(y_true, y_pred)\n#     smooth = 1.\n#     y_true_f = K.flatten(y_true)\n#     y_pred_f = K.flatten(y_pred)\n#     intersection = y_true_f * y_pred_f\n#     score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n#     return 1. - score\n\ndef bce_dice_loss(y_true, y_pred):\n    return 2*binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n\ndef bce_logdice_loss(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) - K.log(1. - dice_loss(y_true, y_pred))\n\ndef weighted_bce_loss(y_true, y_pred, weight):\n    epsilon = 1e-7\n    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n    logit_y_pred = K.log(y_pred / (1. - y_pred))\n    loss = weight * (logit_y_pred * (1. - y_true) + \n                     K.log(1. + K.exp(-K.abs(logit_y_pred))) + K.maximum(-logit_y_pred, 0.))\n    return K.sum(loss) / K.sum(weight)\n\ndef weighted_dice_loss(y_true, y_pred, weight):\n    smooth = 1.\n    w, m1, m2 = weight, y_true, y_pred\n    intersection = (m1 * m2)\n    score = (2. * K.sum(w * intersection) + smooth) / (K.sum(w * m1) + K.sum(w * m2) + smooth)\n    loss = 1. - K.sum(score)\n    return loss\n\ndef weighted_bce_dice_loss(y_true, y_pred):\n    y_true = K.cast(y_true, 'float32')\n    y_pred = K.cast(y_pred, 'float32')\n    # if we want to get same size of output, kernel size must be odd\n    averaged_mask = K.pool2d(\n            y_true, pool_size=(50, 50), strides=(1, 1), padding='same', pool_mode='avg')\n    weight = K.ones_like(averaged_mask)\n    w0 = K.sum(weight)\n    weight = 5. * K.exp(-5. * K.abs(averaged_mask - 0.5))\n    w1 = K.sum(weight)\n    weight *= (w0 / w1)\n    loss = weighted_bce_loss(y_true, y_pred, weight) + dice_loss(y_true, y_pred)\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c9be1f5292b0b5e967ab47603a79651d2037f3a"},"cell_type":"code","source":"import keras.backend as K\nfrom keras.optimizers import Adam\nfrom keras.losses import binary_crossentropy\ndef dice_coef(y_true, y_pred, smooth=1):\n    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n    return K.mean( (2. * intersection + smooth) / (union + smooth), axis=0)\ndef dice_p_bce(in_gt, in_pred):\n    return 1e-3*binary_crossentropy(in_gt, in_pred) - dice_coef(in_gt, in_pred)\ndef true_positive_rate(y_true, y_pred):\n    return K.sum(K.flatten(y_true)*K.flatten(K.round(y_pred)))/K.sum(y_true)\n# def true_positive_rate(y_true, y_pred):\n#     return K.sum(K.flatten(y_true)*K.flatten(K.round(y_pred)))/(K.sum(y_true) + K.epsilon())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# F2 score eval from iafoss submission notebook"},{"metadata":{"trusted":true},"cell_type":"code","source":"def IoU(pred, targs):\n    pred = (pred > 0.5).astype(float)\n    intersection = (pred*targs).sum()\n    return intersection / ((pred+targs).sum() - intersection + 1.0)\n\ndef get_score(pred, true):\n    n_th = 10\n    b = 4\n    thresholds = [0.5 + 0.05*i for i in range(n_th)]\n    n_masks = len(true)\n    n_pred = len(pred)\n    ious = []\n    score = 0\n    for mask in true:\n        buf = []\n        for p in pred: buf.append(IoU(p,mask))\n        ious.append(buf)\n    for t in thresholds:   \n        tp, fp, fn = 0, 0, 0\n        for i in range(n_masks):\n            match = False\n            for j in range(n_pred):\n                if ious[i][j] > t: match = True\n            if not match: fn += 1\n        \n        for j in range(n_pred):\n            match = False\n            for i in range(n_masks):\n                if ious[i][j] > t: match = True\n            if match: tp += 1\n            else: fp += 1\n        score += ((b+1)*tp)/((b+1)*tp + b*fn + fp)       \n    return score/n_th\n\ndef split_mask(mask):\n    \"\"\"\n     return score/n_th\nIn this competition we should submit and individual mask for each identified ship. The simplest way to do it is splitting the total mask into individual ones based on the connectivity of detected objects.\n\"\"\"\n    threshold = 0.5\n    threshold_obj = 30 #ignor predictions composed of \"threshold_obj\" pixels or less\n    labled,n_objs = ndimage.label(mask > threshold)\n    result = []\n    for i in range(n_objs):\n        obj = (labled == i + 1).astype(int)\n        if(obj.sum() > threshold_obj): result.append(obj)\n    return result\n\ndef get_mask_ind(img_id, df, shape = (768,768)): #return mask for each ship\n    masks = df.loc[img_id]['EncodedPixels']\n    if(type(masks) == float): return []\n    if(type(masks) == str): masks = [masks]\n    result = []\n    for mask in masks:\n        img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n        s = mask.split()\n        for i in range(len(s)//2):\n            start = int(s[2*i]) - 1\n            length = int(s[2*i+1])\n            img[start:start+length] = 1\n        result.append(img.reshape(shape).T)\n    return result\n\nclass Score_eval():\n    def __init__(self):\n        self.segmentation_df = pd.read_csv(SEGMENTATION).set_index('ImageId')\n        self.score, self.count = 0.0, 0\n        \n    def put(self,pred,name):\n        true = get_mask_ind(name, self.segmentation_df)\n        self.score += get_score(pred,true)\n        self.count += 1\n        \n    def evaluate(self):\n        return self.score/self.count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## F2 eval score callback"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import Callback\n\n\n# Notice that this callback only works with Keras 2.0.0\nSTART = 0.5\nEND = 0.95\nSTEP = 0.05\nN_STEPS = int((END - START) / STEP) + 2\nDEFAULT_THRESHOLDS = np.linspace(START, END, N_STEPS)\nDEFAULT_LOGS = {}\nFBETA_METRIC_NAME = \"val_f2\"\n\nfrom keras import backend as K\n\n\nclass FBetaMetricCallback(Callback):\n\n    def __init__(self, validation_generator, validation_steps, beta=2, thresholds=DEFAULT_THRESHOLDS):\n        self.validation_generator = validation_generator\n        self.validation_steps = validation_steps\n        self.beta = beta\n        self.thresholds = thresholds\n        # Will be initialized when the training starts\n        self.val_fbeta = None\n\n    def on_train_begin(self, logs=DEFAULT_LOGS):\n        \"\"\" This is where the validation Fbeta\n        validation scores will be saved during training: one value per\n        epoch.\n        \"\"\"\n        self.val_fbeta = []\n            \n    def _fbeta_score_OLD(self, y_true, y_pred, beta, threshold_shift=0):\n        beta = 2\n\n        # just in case of hipster activation at the final layer\n        y_pred = K.clip(y_pred, 0, 1)\n\n        # shifting the prediction threshold from .5 if needed\n        y_pred_bin = K.round(y_pred + threshold_shift)\n\n        tp = K.sum(K.round(y_true * y_pred_bin)) + K.epsilon()\n        fp = K.sum(K.round(K.clip(y_pred_bin - y_true, 0, 1)))\n        fn = K.sum(K.round(K.clip(y_true - y_pred, 0, 1)))\n\n        precision = tp / (tp + fp)\n        recall = tp / (tp + fn)\n\n        beta_squared = beta ** 2\n        return (beta_squared + 1) * (precision * recall) / (beta_squared * precision + recall + K.epsilon())\n\n\n    def _score_per_threshold(self, predictions, targets, threshold):\n        \"\"\" Compute the Fbeta score per threshold.\n        \"\"\"\n        # Notice that here I am using the sklearn fbeta_score function.\n        # You can read more about it here:\n        # http://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html\n        thresholded_predictions = (predictions > threshold).astype('int32')\n        thresholded_predictions = thresholded_predictions.flatten()\n        targets = targets.flatten()\n\n        return fbeta_score(targets, thresholded_predictions, average=None, beta=self.beta)\n#     return _fbeta_score(targets, thresholded_predictions, beta=self.beta)\n\n    def on_epoch_end(self, epoch, logs=DEFAULT_LOGS):\n        val_predictions = self.model.predict_generator(self.validation_generator, steps=self.validation_steps)#,steps = nb_samples)\n        \n        val_targets = []\n        for i in range(self.validation_steps):\n            val_targets.extend(next(iter(self.validation_generator))[1])\n        val_targets = np.array(val_targets)\n#         val_targets = self.validation_data[1]\n        \n        _val_fbeta = get_score(val_predictions, val_targets)\n#         _val_fbeta = np.mean([self._score_per_threshold(val_predictions,\n#                                                         val_targets, threshold)\n#                               for threshold in self.thresholds])\n        self.val_fbeta.append(_val_fbeta)\n        print(\"Current F{} metric is: {}\".format(str(self.beta), str(_val_fbeta)))\n        return\n\n    def on_train_end(self, logs=DEFAULT_LOGS):\n        \"\"\" Assign the validation Fbeta computed metric to the History object.\n        \"\"\"\n        self.model.history.history[FBETA_METRIC_NAME] = self.val_fbeta\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LRFinder"},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras.backend as K\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass LRFinder(Callback):\n    def __init__(self, min_lr, max_lr, mom=0.9, stop_multiplier=None, \n                 reload_weights=True, batches_lr_update=5):\n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.mom = mom\n        self.reload_weights = reload_weights\n        self.batches_lr_update = batches_lr_update\n        if stop_multiplier is None:\n            self.stop_multiplier = -20*self.mom/3 + 10 # 4 if mom=0.9\n                                                       # 10 if mom=0\n        else:\n            self.stop_multiplier = stop_multiplier\n        \n    def on_train_begin(self, logs={}):\n        p = self.params\n        try:\n            n_iterations = p['epochs']*p['samples']//p['batch_size']\n        except:\n            n_iterations = p['steps']*p['epochs']\n            \n        self.learning_rates = np.geomspace(self.min_lr, self.max_lr, \\\n                                           num=n_iterations//self.batches_lr_update+1)\n        self.losses=[]\n        self.iteration=0\n        self.best_loss=0\n        if self.reload_weights:\n            self.model.save_weights('tmp.hdf5')\n        \n    \n    def on_batch_end(self, batch, logs={}):\n        loss = logs.get('loss')\n        \n        if self.iteration!=0: # Make loss smoother using momentum\n            loss = self.losses[-1]*self.mom+loss*(1-self.mom)\n        \n        if self.iteration==0 or loss < self.best_loss: \n                self.best_loss = loss\n                \n        if self.iteration%self.batches_lr_update==0: # Evaluate each lr over 5 epochs\n            \n            if self.reload_weights:\n                self.model.load_weights('tmp.hdf5')\n          \n            lr = self.learning_rates[self.iteration//self.batches_lr_update]            \n            K.set_value(self.model.optimizer.lr, lr)\n\n            self.losses.append(loss)            \n\n        if loss > self.best_loss*self.stop_multiplier: # Stop criteria\n            self.model.stop_training = True\n                \n        self.iteration += 1\n    \n    def on_train_end(self, logs=None):\n        if self.reload_weights:\n                self.model.load_weights('tmp.hdf5')\n                \n        plt.figure(figsize=(12, 6))\n        plt.plot(self.learning_rates[:len(self.losses)], self.losses)\n        plt.xlabel(\"Learning Rate\")\n        plt.ylabel(\"Loss\")\n        plt.xscale('log')\n        plt.show()\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5149d0e0cf06775123a0c661bb5ea3c2fd818dc1"},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a9f308bebfc7a8f067ca61bde886d33bd0e4649"},"cell_type":"markdown","source":"## Set Training Check Point"},{"metadata":{"trusted":true,"_uuid":"3128f14c1f1b86e8b383caea926b5f16d4f5094b"},"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nweight_path=\"{}_weights.best.hdf5\".format('seg_model')\n\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_dice_coef', verbose=1, \n                             save_best_only=True, mode='max', save_weights_only = True)\n\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_dice_coef', factor=0.5, \n                                   patience=3, \n                                   verbose=1, mode='max', epsilon=0.0001, cooldown=2, min_lr=1e-6)\nearly = EarlyStopping(monitor=\"val_dice_coef\", \n                      mode=\"max\", \n                      patience=15) # probably needs to be more patient, but kaggle time is limited\n\n#callbacks\n# callbacks_list = [checkpoint, early, reduceLROnPlat]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50df57cf454854540c1594f0ed7118d3d0427470"},"cell_type":"markdown","source":"## Compile model"},{"metadata":{"trusted":true,"_uuid":"d7d7d7203c83a918386335584b8a0f378a48bfff"},"cell_type":"code","source":"model = UResNet34(input_shape=(768, 768, 3),encoder_weights=True)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights(PRETRAINED_SEG_MODEL_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.save('seg_model_768_ships_ratio_0741_5hr.h5')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d09b8deb89b59f2dfb10e664c34dd9514ec0a06d"},"cell_type":"markdown","source":"## Start Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"# training params\ninput_size = 768#256\nbatch_size = 10#64\nships_ratio = None # TO CHANGE\n\nlr = 1e-7# for Adam   3-4  *1e-4 for SGD (bad LRFINDER graph) [256px/64:1e-4 Adam, 384px/32:3*1e-5 Adam, 768/10:1e-5\nepochs = 5\nloss = bce_logdice_loss\n\nMAX_TRAIN_STEPS = int(train_df.shape[0]/batch_size/15) # num of batches per epoch # !!!!!!!! /20 instead of /10 for LRFinder\nMAX_VAL_STEPS = 100\n\ntraining_steps = min(MAX_TRAIN_STEPS, train_df.shape[0]//batch_size)\nvalidation_steps = min(MAX_VAL_STEPS, valid_df.shape[0]//batch_size)\nprint(\"training_steps:\", training_steps)\nprint(\"validation_steps:\", validation_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# yes=0\n# list_l = [1 if layer.trainable==True else 0 for layer in model.layers]\n# sum(list_l)/len(list_l)\n\nfor layer in model.layers:\n    layer.trainable = True\n    if layer.trainable is False:\n        print(\"layer:\", layer, \"layer name:\", layer.name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_lrs = {\n    \"ships_ratio=1\": \n    {\n        'epochs': 1,\n        'freezed encoder': 1e-4\n        \n    }\n    \n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ntraining:\n1 epoch with backbone=freezed, (lr=1e-4), ships_ratio=1 (~300 train steps here and below ) 17 mins ENOUGH (max perf:: loss: 0.3784 - dice_coef: 0.5345)\n1 epoch with backbone=unfreezed, (lr=1e-4), ships_ratio=1 -> Epoch 1/1 217/217 [==============================] - 255s 1s/step - loss: 1.1537 - dice_coef: 0.2762 - binary_accuracy: 0.9551 - true_positive_rate: 0.6371 - val_loss: 0.4210 - val_dice_coef: 0.4809 - val_binary_accuracy: 0.9858 - val_true_positive_rate: 0.6577\nOR 5 epochs with unfreezed, (lr=1e-4), ships_ratio=1 ,(256px, 64bs) (MAX_TRAIN_STEPS=/10) 0:53 start (~15 min epoch)\nOR Epoch 4/10\n347/348 [============================>.] - ETA: 2s - loss: 0.1832 - dice_coef: 0.6832 - binary_accuracy: 0.9923 - true_positive_rate: 0.8557\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"837b6e7589a725875f1d0ddfd410e8bd88989e2b"},"cell_type":"code","source":"# lr_finder = LRFinder(min_lr=5*1e-5, max_lr=8*1e-3, mom=0.9)\n# callbacks_list = [lr_finder]\ncallbacks_list = [checkpoint, early, reduceLROnPlat]\n\n\n# Initial loss was bce_logdice_loss\n\n#change input shape for data and the model\nmodel, train_generator, validation_generator = change_input_shape(model, loss, lr=lr, new_input_size=input_size, batch_size=batch_size, ships_ratio=ships_ratio)\n\n# fbeta_metric_callback = FBetaMetricCallback(validation_generator, int(validation_steps/10), beta=2)\n# callbacks_list = [fbeta_metric_callback, checkpoint, early, reduceLROnPlat] #  calculates f_beta score just on pixels not based on IOU\n\n#training\nprint(\"Start training...\")\nfrom datetime import datetime\nprint(\"time start model's training: \", datetime.now())\nloss_history = model.fit_generator(\n    train_generator,\n    validation_data=validation_generator,\n    epochs=epochs,\n    callbacks=callbacks_list,\n    steps_per_epoch=training_steps,\n    validation_steps=validation_steps,\n    shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('seg_model_768_sr_0741_5hr.h5')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# -------------------------------------------------------   SHIPS RATIO = 1\n# FREEZED\n# 271/348 [======================>.......] - ETA: 3:12 - \n#         loss: 0.3791 - dice_coef: 0.5334 - binary_accuracy: 0.9854 - true_positive_rate: 0.7318\n\n# UNFREEZED\n# Changing model's input shape to: 256\n# Creating datagenerators...\n# is_validation: False, ships_ratio:1\n# is_validation: True, ships_ratio:1\n# Start training...\n# time start model's training:  2020-02-23 21:45:36.721266  \n# Epoch 1/10\n#  14/348 [>.............................] - ETA: 20:48 - loss: 0.4108 - dice_coef: 0.5399 - binary_accuracy: 0.9840 - true_positive_rate: 0.7253\n# Epoch 1/10\n# 348/348 [==============================] - 1029s 3s/step - loss: 0.2719 - dice_coef: 0.6001 - binary_accuracy: 0.9891 - true_positive_rate: 0.7974 - val_loss: 0.2483 - val_dice_coef: 0.6257 - val_binary_accuracy: 0.9902 - val_true_positive_rate: 0.7909\n\n# Epoch 00001: val_dice_coef improved from 0.52284 to 0.62570, saving model to seg_model_weights.best.hdf5\n# Epoch 2/10\n# 348/348 [==============================] - 979s 3s/step - loss: 0.2152 - dice_coef: 0.6430 - binary_accuracy: 0.9912 - true_positive_rate: 0.8324 - val_loss: 0.1989 - val_dice_coef: 0.6793 - val_binary_accuracy: 0.9910 - val_true_positive_rate: 0.8540\n\n# Epoch 00002: val_dice_coef improved from 0.62570 to 0.67928, saving model to seg_model_weights.best.hdf5\n# Epoch 3/10\n# 348/348 [==============================] - 988s 3s/step - loss: 0.1982 - dice_coef: 0.6620 - binary_accuracy: 0.9919 - true_positive_rate: 0.8455 - val_loss: 0.1788 - val_dice_coef: 0.7091 - val_binary_accuracy: 0.9916 - val_true_positive_rate: 0.8854\n\n# Epoch 00003: val_dice_coef improved from 0.67928 to 0.70910, saving model to seg_model_weights.best.hdf5\n# Epoch 4/10\n# 348/348 [==============================] - 999s 3s/step - loss: 0.1833 - dice_coef: 0.6831 - binary_accuracy: 0.9923 - true_positive_rate: 0.8554 - val_loss: 0.2697 - val_dice_coef: 0.6966 - val_binary_accuracy: 0.9923 - val_true_positive_rate: 0.8651\n\n# Epoch 00004: val_dice_coef did not improve from 0.70910\n# Epoch 5/10\n# 348/348 [==============================] - 1012s 3s/step - loss: 0.1759 - dice_coef: 0.6891 - binary_accuracy: 0.9928 - true_positive_rate: 0.8611 - val_loss: 0.1652 - val_dice_coef: 0.7154 - val_binary_accuracy: 0.9926 - val_true_positive_rate: 0.8684\n\n# Epoch 00005: val_dice_coef improved from 0.70910 to 0.71536, saving model to seg_model_weights.best.hdf5\n# Epoch 6/10\n# 348/348 [==============================] - 1010s 3s/step - loss: 0.1684 - dice_coef: 0.6968 - binary_accuracy: 0.9928 - true_positive_rate: 0.8675 - val_loss: 0.1481 - val_dice_coef: 0.7087 - val_binary_accuracy: 0.9930 - val_true_positive_rate: 0.8580\n\n# Epoch 00006: val_dice_coef did not improve from 0.71536\n\n# 23:27 Keybord Interrupt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MODEL AFTER TRAINING WITH ships_ratio=1 with results described in the cell above\n\n# ships_ratio images 0.741\n\n# time start model's training:  2020-02-24 19:50:04.443397\n# Epoch 1/20\n# 723/723 [==============================] - 2070s 3s/step - loss: 0.2694 - dice_coef: 0.7855 - binary_accuracy: 0.9937 - true_positive_rate: 0.8700 - val_loss: 0.1742 - val_dice_coef: 0.8564 - val_binary_accuracy: 0.9973 - val_true_positive_rate: 0.8982\n# Current F2 metric is: 0.004880824940946163\n\n# Epoch 00001: val_dice_coef improved from -inf to 0.85642, saving model to seg_model_weights.best.hdf5\n# Epoch 2/20\n# 723/723 [==============================] - 1986s 3s/step - loss: 0.2576 - dice_coef: 0.7941 - binary_accuracy: 0.9940 - true_positive_rate: 0.8743 - val_loss: 0.1090 - val_dice_coef: 0.9217 - val_binary_accuracy: 0.9982 - val_true_positive_rate: 0.8664\n# Current F2 metric is: 0.006318825231516655\n\n# Epoch 00002: val_dice_coef improved from 0.85642 to 0.92169, saving model to seg_model_weights.best.hdf5\n# Epoch 3/20\n# 592/723 [=======================>......] - ETA: 5:36 - loss: 0.2769 - dice_coef: 0.7813 - binary_accuracy: 0.9936 - true_positive_rate: 0.8656\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Changing model's input shape to: 768                       0.3630       0.3307\n# Creating datagenerators...\n# is_validation: False, ships_ratio:0.742\n# is_validation: True, ships_ratio:0.22\n# Start training...\n# time start model's training:  2020-02-25 13:47:57.119213\n# Epoch 1/5\n#  25/604 [>.............................] - ETA: 19:58 - loss: 0.3575 - dice_coef: 0.7090 - binary_accuracy: 0.9984 - true_positive_rate: 0.8727\n# 119/604 [====>.........................] - ETA: 11:17 - loss: 0.3973 - dice_coef: 0.6858 - binary_accuracy: 0.9983 - true_positive_rate: 0.8626\n\n# Epoch 1/5\n# 604/604 [==============================] - 811s 1s/step - loss: 0.3948 - dice_coef: 0.6890 - binary_accuracy: 0.9984 - true_positive_rate: 0.8335 - val_loss: 0.1545 - val_dice_coef: 0.7280 - val_binary_accuracy: 0.9995 - val_true_positive_rate: nan\n\n# Epoch 00001: val_dice_coef improved from -inf to 0.72802, saving model to seg_model_weights.best.hdf5\n# Epoch 2/5\n# 604/604 [==============================] - 753s 1s/step - loss: 0.3634 - dice_coef: 0.7078 - binary_accuracy: 0.9987 - true_positive_rate: 0.8170 - val_loss: 0.3391 - val_dice_coef: 0.7608 - val_binary_accuracy: 0.9996 - val_true_positive_rate: nan\n\n# Epoch 00002: val_dice_coef improved from 0.72802 to 0.76078, saving model to seg_model_weights.best.hdf5\n# Epoch 3/5\n#  59/604 [=>............................] - ETA: 9:35 - loss: 0.3333 - dice_coef: 0.7294 - binary_accuracy: 0.9988 - true_positive_rate: 0.8525","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_history.history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4438fd660d8ef48310e890415da0b549971b9884"},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce1167e9f09200f537e61f93f486168a13be1711"},"cell_type":"code","source":"# seg_model.load_weights(weight_path)\n# seg_model.save('seg_model.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Vizualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"def Show_images(x,yp,yt):\n    columns = 3\n    rows = min(batch_size, 16)\n    fig=plt.figure(figsize=(columns*4, rows*4))\n    for i in range(rows):\n        fig.add_subplot(rows, columns, 3*i+1)\n        plt.axis('off')\n        plt.imshow(x[i])\n        fig.add_subplot(rows, columns, 3*i+2)\n        plt.axis('off')\n        plt.imshow(yp[i][:,:,0], cmap='gray')\n        fig.add_subplot(rows, columns, 3*i+3)\n        plt.axis('off')\n        plt.imshow(yt[i][:,:,0], cmap='gray')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 5\nvis_generator = train_generator \nvis_generator = validation_generator\nX, y = vis_generator.__getitem__(i)\ny_pred = model.predict(X)\n\nShow_images(X, y_pred, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 1\nvis_generator = train_generator \n# vis_generator = validation_generator\nX, y = vis_generator.__getitem__(i)\ny_pred = model.predict(X)\n\nShow_images(X, y_pred, y)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}